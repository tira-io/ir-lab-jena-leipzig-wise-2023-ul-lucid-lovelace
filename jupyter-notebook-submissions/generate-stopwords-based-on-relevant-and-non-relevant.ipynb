{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Detect if we are in the TIRA sandbox\n",
    "# Install the required dependencies if we are not in the sandbox.\n",
    "if 'TIRA_DATASET_ID' not in os.environ:\n",
    "    !pip3 install python-terrier tira==0.0.88 ir_datasets trectools\n",
    "else:\n",
    "    print('We are in the TIRA sandbox.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "print('importing libraries...')\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "ensure_pyterrier_is_loaded()\n",
    "\n",
    "print('Done. Libraries imported.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRetrievedDocumentIds(qrels, run, k, is_relevant):\n",
    "\n",
    "    queryIdIndex = 0\n",
    "    docIdIndex = 2    \n",
    "    relevanceIndex = 3\n",
    "\n",
    "    relevant_documents = [item for item in qrels if item[relevanceIndex] == is_relevant]\n",
    "    relevant_retrieved_documents = []\n",
    "\n",
    "    for item in relevant_documents:\n",
    "        query_id, doc_id = item[queryIdIndex], item[docIdIndex]\n",
    "        is_document_retrieved_in_top_10 = doc_id in run.get_top_documents(query_id, k)\n",
    "        if is_document_retrieved_in_top_10:\n",
    "            relevant_retrieved_documents.append(item[docIdIndex])\n",
    "    \n",
    "    return relevant_retrieved_documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformDocumentIdsToDocuments(document_id_list, original_documents): \n",
    "    relevant_retrieved_documents = []\n",
    "    for document in list(original_documents):\n",
    "        if document.doc_id in document_id_list:\n",
    "            relevant_retrieved_documents.append(document)\n",
    "\n",
    "    return relevant_retrieved_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "\n",
    "def createSortedTermFrequency(relevant_retrieved_documents):\n",
    "    indexer = pt.IterDictIndexer(\n",
    "        \"/tmp/index\",\n",
    "        overwrite=True,\n",
    "        meta={'docno': 100, 'text': 20480},\n",
    "        stemmer=None\n",
    "    )\n",
    "    index_ref = indexer.index(({'docno': i.doc_id, 'text': i.text} for i in relevant_retrieved_documents))\n",
    "    index_from_relevant_retrieved = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "    lexicon = index_from_relevant_retrieved.getLexicon()\n",
    "    term_frequencies = [(term, le.getFrequency()/len(relevant_retrieved_documents)) for term, le in lexicon]\n",
    "    sorted_term_frequencies = sorted(term_frequencies, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_term_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_retrieved_documents(training_qrels, run, original_documents, top_k_documents):\n",
    "    qrels = training_qrels.qrels_data.values.tolist()\n",
    "    relevant_retrieved_document_ids = getRetrievedDocumentIds(qrels, run, top_k_documents, 1)\n",
    "    relevant_retrieved_documents = transformDocumentIdsToDocuments(relevant_retrieved_document_ids, original_documents)\n",
    "    sorted_term_frequency = createSortedTermFrequency(relevant_retrieved_documents)\n",
    "\n",
    "    return sorted_term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_relevant_retrieved_documents(training_qrels, run, original_documents, top_k_documents):\n",
    "    qrels = training_qrels.qrels_data.values.tolist()\n",
    "    non_relevant_retrieved_document_ids = getRetrievedDocumentIds(qrels, run, top_k_documents, 0)\n",
    "    non_relevant_retrieved_documents = transformDocumentIdsToDocuments(non_relevant_retrieved_document_ids, original_documents)\n",
    "    sorted_term_frequency = createSortedTermFrequency(non_relevant_retrieved_documents)\n",
    "    \n",
    "    return sorted_term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ir_datasets\n",
    "def get_all_query_terms():\n",
    "    training_dataset = 'ir-lab-jena-leipzig-wise-2023/training-20231104-training'\n",
    "    dataset = ir_datasets.load(training_dataset)\n",
    "\n",
    "    all_query_terms = []\n",
    "\n",
    "    for query in list(dataset.queries_iter()):\n",
    "        text = query.default_text()\n",
    "\n",
    "        text_split = text.split(' ')\n",
    "\n",
    "        all_query_terms = [*all_query_terms, *text_split]\n",
    "\n",
    "    deduplicated_query_terms = set(all_query_terms)\n",
    "    return deduplicated_query_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered(all_stopwords):\n",
    "    return [tup for tup in all_stopwords if tup[1] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_without_query_terms(all_stopwords):\n",
    "    all_query_terms = get_all_query_terms()\n",
    "    return [tup for tup in all_stopwords if tup[0] not in all_query_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_and_without_query_terms(all_stopwords):\n",
    "    filtered = get_filtered(all_stopwords)\n",
    "    return get_without_query_terms(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecRun, TrecQrel, TrecEval\n",
    "from tira.rest_api_client import Client\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "tira = Client()\n",
    "\n",
    "from load_dataset import load_dataset\n",
    "\n",
    "def load_qrels(dataset):\n",
    "    return TrecQrel(tira.download_dataset('ir-lab-jena-leipzig-wise-2023', dataset, truth_dataset=True) + '/qrels.txt')\n",
    "\n",
    "TOP_Ks = [10, 50]\n",
    "\n",
    "for top_k in TOP_Ks:\n",
    "    training_qrels = load_qrels('training-20231104-training')\n",
    "\n",
    "    run = TrecRun('./runs/standard_stopwords/run.txt')\n",
    "    training_dataset = 'ir-lab-jena-leipzig-wise-2023/training-20231104-training'\n",
    "    documents = load_dataset(training_dataset)['documents']\n",
    "\n",
    "    relevant_documents = get_relevant_retrieved_documents(training_qrels, run, documents, top_k)\n",
    "\n",
    "    training_qrels = load_qrels('training-20231104-training')\n",
    "\n",
    "    run = TrecRun('./runs/standard_stopwords/run.txt')\n",
    "    training_dataset = 'ir-lab-jena-leipzig-wise-2023/training-20231104-training'\n",
    "    documents = load_dataset(training_dataset)['documents']\n",
    "    non_relevant_documents = get_non_relevant_retrieved_documents(training_qrels, run, documents, top_k)\n",
    "\n",
    "    result = []\n",
    "    for (word_rel, count_rel), (word_non_rel, count_non_rel) in zip(relevant_documents, non_relevant_documents):\n",
    "        if word_rel == word_non_rel:\n",
    "            result.append((word_rel, count_rel / count_non_rel))\n",
    "\n",
    "    sorted_result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    stopwords_filtered_over_threshold = get_filtered(sorted_result)\n",
    "    stopwords_without_query_terms = get_without_query_terms(sorted_result)\n",
    "    stopwords_filtered_and_without_query_terms = get_filtered_and_without_query_terms(sorted_result)\n",
    "\n",
    "    file_path_original = f\"./stopwordlists/joined-relevant-and-non-relevant-based-stopwords-index/single_top{top_k}.txt\"\n",
    "    file_path1 = f'./stopwordlists/joined-relevant-and-non-relevant-based-stopwords-index/filtered_top{top_k}.txt'\n",
    "    file_path2 = f'./stopwordlists/joined-relevant-and-non-relevant-based-stopwords-index/without_query_terms_top{top_k}.txt'\n",
    "    file_path3 = f'./stopwordlists/joined-relevant-and-non-relevant-based-stopwords-index/filtered_and_without_query_terms_top{top_k}.txt'\n",
    "\n",
    "    with open(file_path_original, 'w') as file:\n",
    "        for string in [word for word, _ in sorted_result]:\n",
    "            file.write(string + '\\n') \n",
    "\n",
    "    with open(file_path1, 'w') as file:\n",
    "        for string in [word for word, _ in stopwords_filtered_over_threshold]:\n",
    "            file.write(string + '\\n') \n",
    "\n",
    "    with open(file_path2, 'w') as file:\n",
    "        for string in [word for word, _ in stopwords_without_query_terms]:\n",
    "            file.write(string + '\\n') \n",
    "\n",
    "    with open(file_path3, 'w') as file:\n",
    "        for string in [word for word, _ in stopwords_filtered_and_without_query_terms]:\n",
    "            file.write(string + '\\n') "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
