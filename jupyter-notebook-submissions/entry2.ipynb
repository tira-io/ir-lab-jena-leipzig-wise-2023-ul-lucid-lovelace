{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Detect if we are in the TIRA sandbox\n",
    "# Install the required dependencies if we are not in the sandbox.\n",
    "if 'TIRA_DATASET_ID' not in os.environ:\n",
    "    !pip3 install python-terrier tira==0.0.88 ir_datasets\n",
    "else:\n",
    "    print('We are in the TIRA sandbox.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "print('importing libraries...')\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "ensure_pyterrier_is_loaded()\n",
    "\n",
    "print('Done. Libraries imported.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "\n",
    "import pyterrier as pt\n",
    "import os\n",
    "\n",
    "def create_index(documents, indexLocation=False):\n",
    "    indexer = pt.IterDictIndexer(\n",
    "        indexLocation if indexLocation else \"/tmp/index\",\n",
    "        overwrite=True,\n",
    "        meta={'docno': 100, 'text': 20480},\n",
    "        stemmer=None\n",
    "    )\n",
    "    index_ref = indexer.index(({'docno': i.doc_id, 'text': i.text} for i in documents))\n",
    "    return pt.IndexFactory.of(index_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "from tira.third_party_integrations import ir_datasets\n",
    "import pyterrier as pt\n",
    "\n",
    "def load_dataset(training_dataset):\n",
    "    queries = pt.io.read_topics(ir_datasets.topics_file(training_dataset), format='trecxml')\n",
    "\n",
    "    dataset = ir_datasets.load(training_dataset)\n",
    "    return {'documents': dataset.docs_iter(), 'queries': queries, 'topics': dataset.queries_iter()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "import pyterrier as pt\n",
    "\n",
    "def create_model(index):\n",
    "    return pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "training_dataset = 'ir-lab-jena-leipzig-wise-2023/training-20231104-training'\n",
    "queries = load_dataset(training_dataset)['queries']\n",
    "\n",
    "stopword_names = [\n",
    "    './stopwordlists/stopwords_english_long.txt',\n",
    "    './stopwordlists/stopwords_improved_merged_top10.txt',\n",
    "    './stopwordlists/stopwords_improved_single_top10.txt',\n",
    "    './stopwordlists/stopwords_improved_single_top50.txt'\n",
    "]\n",
    "\n",
    "for stopwords_path in stopword_names:\n",
    "    if os.path.exists(stopwords_path):\n",
    "        print(f\"The file '{stopwords_path}' exists.\")\n",
    "    else:\n",
    "        print('missing file', stopwords_path)\n",
    "        raise ValueError('stopwords file does not exist')\n",
    "\n",
    "    run_name = stopwords_path.replace('./stopwordlists/stopwords_', '').replace('.txt', '')\n",
    "    output_dir = 'runs/applied-custom-stopwords'\n",
    "    run_output_dir = output_dir + '/' + run_name\n",
    "\n",
    "    pt.set_property(\"stopwords.filename\", stopwords_path)\n",
    "\n",
    "    new_index = create_index(load_dataset(training_dataset)['documents'])\n",
    "    print(\"index created\")\n",
    "\n",
    "    improved_model = create_model(new_index)\n",
    "    print(\"model created\")\n",
    "\n",
    "    run = improved_model(queries)\n",
    "\n",
    "    !rm -Rf {run_output_dir}\n",
    "    !mkdir -p {run_output_dir}\n",
    "\n",
    "    persist_and_normalize_run(run, run_name, run_output_dir)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Significance Test Used Locally\n",
    "\n",
    "import glob\n",
    "from create_index import create_index\n",
    "from load_dataset import load_dataset\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pyterrier as pt\n",
    "from trectools import TrecRun, TrecQrel, TrecEval\n",
    "from tira.rest_api_client import Client\n",
    "import pandas as pd\n",
    "tira = Client()\n",
    "\n",
    "training_dataset = 'ir-lab-jena-leipzig-wise-2023/training-20231104-training'\n",
    "load_dataset_result = load_dataset(training_dataset)\n",
    "\n",
    "qrels_data = pd.read_csv(tira.download_dataset('ir-lab-jena-leipzig-wise-2023', 'training-20231104-training', truth_dataset=True) + '/qrels.txt', sep=\"\\s+\", names=[\"qid\",\"q0\",\"docno\",\"relevance\"])\n",
    "# Enforce string type on docid column (if present)\n",
    "if \"docno\" in qrels_data:\n",
    "    qrels_data[\"docno\"] = qrels_data[\"docno\"].astype(str)\n",
    "# Enforce string type on q0 column (if present)\n",
    "if \"q0\" in qrels_data:\n",
    "    qrels_data[\"q0\"] = qrels_data[\"q0\"].astype(str)\n",
    "# Enforce string type on query column (if present)\n",
    "if \"qid\" in qrels_data:\n",
    "    qrels_data[\"qid\"] = qrels_data[\"qid\"].astype(str)\n",
    "# Removes the files that were not judged:\n",
    "qrels_data = qrels_data[qrels_data[\"relevance\"] >= 0]\n",
    "\n",
    "\n",
    "\n",
    "#training_qrels = load_qrels('training-20231104-training')\n",
    "training_dataset = 'ir-lab-jena-leipzig-wise-2023/training-20231104-training'\n",
    "load_dataset_result = load_dataset(training_dataset)\n",
    "\n",
    "improved_stopwords=\"./stopwordlists/auswertung/stopwords_english_long.txt\"\n",
    "pt.set_property(\"stopwords.filename\",improved_stopwords)\n",
    "normal_index = create_index(load_dataset(training_dataset)['documents'])\n",
    "print(\"index1 created\")\n",
    "normal_stopword_model = create_model(normal_index)\n",
    "print(\"model1 created\")\n",
    "\n",
    "improved_stopwords=\"./stopwordlists/auswertung/stopwords_improved_merged_top10.txt\"\n",
    "pt.set_property(\"stopwords.filename\",improved_stopwords)\n",
    "improved_index = create_index(load_dataset(training_dataset)['documents'])\n",
    "print(\"index2 created\")\n",
    "improved_model = create_model(improved_index)\n",
    "print(\"model2 created\")\n",
    "\n",
    "print(pt.Experiment\n",
    "      (\n",
    "[normal_stopword_model,improved_model],\n",
    "load_dataset_result['queries'],\n",
    "qrels_data,\n",
    "eval_metrics=[\"map\", \"recip_rank\"],\n",
    "names=[\"normal_stopword_model\", \"improved_model\"],\n",
    "baseline=0,\n",
    "correction='bonferroni'))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
